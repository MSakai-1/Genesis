diff --git a/examples/locomotion/go2_eval.py b/examples/locomotion/go2_eval.py
index cdbdc45..b027918 100644
--- a/examples/locomotion/go2_eval.py
+++ b/examples/locomotion/go2_eval.py
@@ -15,7 +15,8 @@ def main():
     parser.add_argument("--ckpt", type=int, default=100)
     args = parser.parse_args()
 
-    gs.init()
+    # gs.init()
+    gs.init(backend=gs.cpu)
 
     log_dir = f"logs/{args.exp_name}"
     env_cfg, obs_cfg, reward_cfg, command_cfg, train_cfg = pickle.load(open(f"logs/{args.exp_name}/cfgs.pkl", "rb"))
@@ -28,12 +29,15 @@ def main():
         reward_cfg=reward_cfg,
         command_cfg=command_cfg,
         show_viewer=True,
+        device="cpu"
     )
 
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
+    # runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
+    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cpu")
     resume_path = os.path.join(log_dir, f"model_{args.ckpt}.pt")
     runner.load(resume_path)
-    policy = runner.get_inference_policy(device="cuda:0")
+    # policy = runner.get_inference_policy(device="cuda:0")
+    policy = runner.get_inference_policy(device="cpu")
 
     obs, _ = env.reset()
     with torch.no_grad():
diff --git a/examples/locomotion/go2_train.py b/examples/locomotion/go2_train.py
index 7c77990..cf0ce92 100644
--- a/examples/locomotion/go2_train.py
+++ b/examples/locomotion/go2_train.py
@@ -142,7 +142,8 @@ def main():
     parser.add_argument("--max_iterations", type=int, default=100)
     args = parser.parse_args()
 
-    gs.init(logging_level="warning")
+    # gs.init(logging_level="warning")
+    gs.init(logging_level="warning", backend=gs.cpu)
 
     log_dir = f"logs/{args.exp_name}"
     env_cfg, obs_cfg, reward_cfg, command_cfg = get_cfgs()
@@ -153,10 +154,12 @@ def main():
     os.makedirs(log_dir, exist_ok=True)
 
     env = Go2Env(
-        num_envs=args.num_envs, env_cfg=env_cfg, obs_cfg=obs_cfg, reward_cfg=reward_cfg, command_cfg=command_cfg
+        # num_envs=args.num_envs, env_cfg=env_cfg, obs_cfg=obs_cfg, reward_cfg=reward_cfg, command_cfg=command_cfg
+        num_envs=args.num_envs, env_cfg=env_cfg, obs_cfg=obs_cfg, reward_cfg=reward_cfg, command_cfg=command_cfg, device="cpu"
     )
 
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
+    # runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
+    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cpu")
 
     pickle.dump(
         [env_cfg, obs_cfg, reward_cfg, command_cfg, train_cfg],
diff --git a/examples/tutorials/control_your_robot.py b/examples/tutorials/control_your_robot.py
index 72a9fad..80fd797 100644
--- a/examples/tutorials/control_your_robot.py
+++ b/examples/tutorials/control_your_robot.py
@@ -3,7 +3,8 @@ import numpy as np
 import genesis as gs
 
 ########################## init ##########################
-gs.init(backend=gs.gpu)
+# gs.init(backend=gs.gpu)
+gs.init(backend=gs.cpu)
 
 ########################## create a scene ##########################
 scene = gs.Scene(
diff --git a/examples/tutorials/parallel_simulation.py b/examples/tutorials/parallel_simulation.py
index d90f85a..7eec884 100644
--- a/examples/tutorials/parallel_simulation.py
+++ b/examples/tutorials/parallel_simulation.py
@@ -3,5 +3,6 @@ import torch
 import genesis as gs
 
 ########################## init ##########################
-gs.init(backend=gs.gpu)
+# gs.init(backend=gs.gpu)
+gs.init(backend=gs.cpu)
 
